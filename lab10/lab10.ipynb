{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe():\n",
    "    def __init__(self,player = 'X',reward_type ='goal_reward'):\n",
    "        '''\n",
    "        player: Role agent should play. If X, agent has the first turn else agent has second turn\n",
    "        reward_type: 'goal_reward' or 'action_penalty'\n",
    "        '''\n",
    "        self.board = np.array(['__']*9).reshape(3,3)\n",
    "        self.reward_type = reward_type\n",
    "        self.winning_seqeunce = None #Keep track of winning move made by agent\n",
    "        self.first_move = None #Keep track of first move made by agent\n",
    "        if player == 'X':\n",
    "            self.me ='X'\n",
    "            self.id = 1\n",
    "            self.opponent = 'O'\n",
    "        else:\n",
    "            self.me = 'O'\n",
    "            self.id = 2\n",
    "            self.opponent = 'X'\n",
    "     \n",
    "        self.game_over = False #Flag indicating whether game is over\n",
    "        # Mapping of action representation in board to action representation in tuple \n",
    "        self.b_to_s = {'__':0,'X':1,'O':2} \n",
    "        # Mapping of action representation in tuple to action representation in board\n",
    "        self.s_to_b = {0:'__',1:'X',2:'O'} \n",
    "        \n",
    "        #Create mapping from 2D position in board to 1D position in tuple\n",
    "        positions = self.available_positions()\n",
    "        self.b2_to_s1 = {position:i for (i,position) in enumerate(positions)}\n",
    "        \n",
    "        #Create mapping from 1D position in tuple to 2D position in board \n",
    "        self.s1_to_b2 = {i:position for (i,position) in enumerate(positions)}\n",
    "        \n",
    "        #State the current player is in\n",
    "        self.starting_state = self.board_to_state()\n",
    "        \n",
    "        #Initialize all possible states of the game\n",
    "        l_o_l = [list(range(3)) for _ in range(9)]\n",
    "        states = set(product(*l_o_l))\n",
    "        \n",
    "\n",
    "        \n",
    "        #Player X states include states with odd number of blanks and both players have occupied equal number of slots\n",
    "        #Player O players after Player X, so player O states include states with even number of blanks and where\n",
    "        #player X has occupied one more slot than player O\n",
    "        playerX_states = {state for state in states if (state.count(0)%2 == 1 and state.count(1)==state.count(2))} #\n",
    "        playerO_states =  {state for state in states if (state.count(0)%2 == 0 and state.count(1)==(state.count(2)+1))}\n",
    "        \n",
    "        #States \n",
    "        #self.board_full_states = {state for state in states if state.count(0)==0}\n",
    "        if player == 'X':\n",
    "            self.my_states = playerX_states\n",
    "        else:\n",
    "            self.my_states = playerO_states\n",
    "          \n",
    "    \n",
    "    def reset_board(self):\n",
    "        \"Function to reset game and reset board to starting state\"\n",
    "        self.board = np.array(['__']*9).reshape(3,3)\n",
    "        self.starting_state = self.board_to_state()\n",
    "        self.game_over = False\n",
    "        self.winning_sequence = None\n",
    "        self.first_move = None\n",
    "    \n",
    "    def show_board(self):    \n",
    "        \"Shows board as a pandas dataframe\"\n",
    "        return pd.DataFrame(self.board)\n",
    "    \n",
    "    def board_to_state(self):\n",
    "        \"Convert a board to a state in tuple format\"\n",
    "        return tuple([self.b_to_s[x] for x in np.ravel(self.board)])\n",
    "    \n",
    "    @staticmethod\n",
    "    def possible_actions(state):\n",
    "        \"Return possible actions given a state\"\n",
    "        return [i for i,x  in enumerate(state) if x ==0]\n",
    "    \n",
    "\n",
    "        \n",
    "    def is_game_over(self):\n",
    "        \"Function to check if game is over\"\n",
    "        if not np.any(self.board == '__') :\n",
    "            self.game_over = True\n",
    "            \n",
    "        return self.game_over\n",
    "    \n",
    "    def available_positions(self):\n",
    "        \"Return available positions on the board\"\n",
    "        x,y = np.where(self.board =='__')\n",
    "        return[(x,y) for x,y in zip(x,y)]\n",
    "    \n",
    "    \n",
    "    def win(self,player):\n",
    "        \"Check if player won the game and record the winning sequence\"\n",
    "        if np.all(self.board[0,:] == player):\n",
    "            self.winning_sequence = 'R1'\n",
    "        elif np.all(self.board[1,:] == player): \n",
    "            self.winning_sequence = 'R2'\n",
    "        elif np.all(self.board[2,:] == player):\n",
    "            self.winning_sequence = 'R3'\n",
    "        elif np.all(self.board[:,0] == player):\n",
    "            self.winning_sequence = 'C1'\n",
    "        elif np.all(self.board[:,1] == player):\n",
    "            self.winning_sequence = 'C2'\n",
    "        elif np.all(self.board[:,2] == player):\n",
    "            self.winning_sequence = 'C3'\n",
    "        elif np.all(self.board.diagonal()==player):\n",
    "            self.winning_sequence = 'D1'\n",
    "        elif  np.all(np.fliplr(self.board).diagonal()==player):\n",
    "            self.winning_sequence = 'D2'\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def my_move(self,position):\n",
    "        \"Fills out the board in the given position with the action of the agent\"\n",
    "        \n",
    "        assert position[0] >= 0 and position[0] <= 2 and position[1] >= 0 and position[1] <= 2 , \"incorrect position\"\n",
    "        assert self.board[position] == \"__\" , \"position already filled\"\n",
    "        assert np.any(self.board == '__') , \"Board is complete\"\n",
    "        assert self.win(self.me) == False and self.win(self.opponent)== False , \" Game has already been won\"\n",
    "        self.board[position] = self.me\n",
    "        \n",
    "        I_win = self.win(self.me)\n",
    "        opponent_win = self.win(self.opponent)\n",
    "        \n",
    "        if self.reward_type == 'goal_reward':\n",
    "            if I_win:\n",
    "                self.game_over = True\n",
    "                return 1\n",
    "            \n",
    "            elif opponent_win:\n",
    "                self.game_over = True\n",
    "                return -1\n",
    "            \n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        elif self.reward_type == 'action_penalty':\n",
    "            if I_win:\n",
    "                self.game_over = True\n",
    "                return 0\n",
    "            \n",
    "            elif opponent_win:\n",
    "                self.game_over = True\n",
    "                return -10\n",
    "            \n",
    "            else:\n",
    "                return -1\n",
    "    \n",
    "    def opponent_move(self,position):\n",
    "        \"Fills out the board in the given position with the action of the opponent\"\n",
    "        assert position[0] >= 0 and position[0] <= 2 and position[1] >= 0 and position[1] <= 2 , \"incorrect position\"\n",
    "        assert self.board[position] == \"__\" , \"position already filled\"\n",
    "        assert np.any(self.board == '__') , \"Board is complete\"\n",
    "        assert self.win(self.me) == False and self.win(self.opponent)== False , \" Game has already been won\"\n",
    "        self.board[position] = self.opponent\n",
    "            \n",
    "    \n",
    "    def pick_best_action(self,Q,action_type,eps=None):\n",
    "        '''Given a Q function return optimal action\n",
    "        If action_type is 'greedy' return best action with ties broken randomly else return epsilon greedy action\n",
    "        '''\n",
    "        #Get possible actions\n",
    "        current_state = self.board_to_state()\n",
    "        actions =  self.possible_actions(current_state)\n",
    "        \n",
    "        best_action = []\n",
    "        best_action_value = -np.Inf\n",
    "        \n",
    "        for action in actions:\n",
    "            Q_s_a = Q[current_state][action]\n",
    "            if Q_s_a == best_action_value:\n",
    "                best_action.append(action)\n",
    "            elif Q_s_a > best_action_value:\n",
    "                best_action = [action]\n",
    "                best_action_value = Q_s_a\n",
    "        best_action = random.choice(best_action)\n",
    "\n",
    "        if action_type == 'greedy':\n",
    "            return self.s1_to_b2[best_action]\n",
    "        else:\n",
    "            assert eps != None , \"Include epsilon parameter\"\n",
    "            n_actions =len(actions) #No of legal actions \n",
    "            p = np.full(n_actions,eps/n_actions)\n",
    "            #Get index of best action\n",
    "            best_action_i = actions.index(best_action)\n",
    "            p[best_action_i]+= 1 - eps\n",
    "            return self.s1_to_b2[np.random.choice(actions,p=p)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(n_games, Q_X, Q_O, X_strategy='eps_greedy', O_strategy='eps_greedy', eps_X=0.05, eps_O=0.05, seed=1):\n",
    "    np.random.seed(seed)\n",
    "    win_stats = defaultdict(int)\n",
    "    winning_X_first_actions = []\n",
    "    winning_O_first_actions = []\n",
    "\n",
    "    t_board_X = TicTacToe(player='X', reward_type='action_penalty')\n",
    "    t_board_O = TicTacToe(player='O', reward_type='action_penalty')\n",
    "\n",
    "    for i in tqdm(range(n_games), position=0, leave=True):\n",
    "        first_action_flag = True\n",
    "        while True:\n",
    "            x_action = t_board_X.pick_best_action(Q_X, action_type=X_strategy, eps=eps_X)\n",
    "            if first_action_flag:\n",
    "                winning_X_first_actions.append(x_action)\n",
    "\n",
    "            t_board_X.my_move(x_action)\n",
    "            t_board_O.opponent_move(x_action)\n",
    "\n",
    "            if t_board_X.is_game_over():\n",
    "                break\n",
    "\n",
    "            o_action = t_board_O.pick_best_action(Q_O, action_type=O_strategy, eps=eps_O)\n",
    "            if first_action_flag:\n",
    "                winning_O_first_actions.append(o_action)\n",
    "                first_action_flag = False\n",
    "\n",
    "            t_board_O.my_move(o_action)\n",
    "            t_board_X.opponent_move(o_action)\n",
    "\n",
    "            if t_board_O.is_game_over():\n",
    "                break\n",
    "\n",
    "        if t_board_X.win('X'):\n",
    "            win_stats['X_win'] += 1\n",
    "        elif t_board_X.win('O'):\n",
    "            win_stats['O_win'] += 1\n",
    "        else:\n",
    "            win_stats['Draw'] += 1\n",
    "\n",
    "        t_board_X.reset_board()\n",
    "        t_board_O.reset_board()\n",
    "\n",
    "    # Find the most frequent initial winning moves for each player\n",
    "    most_common_X = Counter(winning_X_first_actions).most_common(1)\n",
    "    most_common_O = Counter(winning_O_first_actions).most_common(1)\n",
    "\n",
    "    return win_stats, most_common_X[0][0] if most_common_X else None, most_common_O[0][0] if most_common_O else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q(S,seed = 1):\n",
    "    \"Given a state assign random values to each possible action\"\n",
    "    np.random.seed(seed)\n",
    "    Q = {}\n",
    "    for state in S:\n",
    "        Q[state]= {}\n",
    "        for i,x  in enumerate(state): # Loop through action\n",
    "            if x == 0:\n",
    "                Q[state][i] = np.random.rand()\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_games=1000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=True,**kwargs):\n",
    "    \"\"\"\n",
    "    Function to train a player in a game of tic-tac-toe\n",
    "    Arguments:\n",
    "        n_games: Number of games on which to train\n",
    "        alpha: Learning rate\n",
    "        gamma: discount factor\n",
    "        train_X: Flag indicating whether player X should be trained\n",
    "        train_O: Flag inficating whether player O should be trained\n",
    "        is_random: should actions of untrained agent be random or deterministic according to Q table\n",
    "    \n",
    "    \"\"\"\n",
    "     \n",
    "    \n",
    "    # If Q is not provided, randomize intially, if provided, it will be used to select actions greedily\n",
    "    if \"Q_X\" in kwargs:\n",
    "        action_type_X = \"greedy\"\n",
    "        assert train_X == False ,\"Train flag should be set to False if Q table is being provided\"\n",
    "        Q_X = kwargs[\"Q_X\"]\n",
    "    else:\n",
    "        Q_X = initialize_Q(States_X)\n",
    "        \n",
    "    if \"Q_O\" in kwargs:\n",
    "        action_type_O = \"greedy\"\n",
    "        assert train_O == False ,\"Train flag should be set to False if Q table is being provided\"\n",
    "        Q_O = kwargs[\"Q_O\"]\n",
    "    else:\n",
    "        Q_O = initialize_Q(States_O)\n",
    "    \n",
    "    \n",
    "    #Set epsilon value conditional on whether we are training X or O\n",
    "    eps_ = lambda flag,i: 0.05*0.99**i if flag else 1.0\n",
    "    \n",
    "    \n",
    "    #Lists to keep track of rewards earned by both players during training\n",
    "    \n",
    "    rewards_X = []\n",
    "    rewards_O = []\n",
    "    \n",
    "    \n",
    "    if train_X:\n",
    "        X_action_type = 'eps_greedy'\n",
    "    else:\n",
    "        X_action_type = 'greedy'\n",
    "        if is_random:\n",
    "            X_action_type = 'eps_greedy'\n",
    "        \n",
    "            \n",
    "    \n",
    "    if train_O:\n",
    "        O_action_type = 'eps_greedy'\n",
    "    else:\n",
    "        O_action_type = 'greedy'\n",
    "        if is_random:\n",
    "            O_action_type = 'eps_greedy'\n",
    "            \n",
    "    for i in tqdm(range(n_games),position=0,leave=True):\n",
    "        \n",
    "        eps = 0.05*0.99**i\n",
    "        t_board_X.reset_board()\n",
    "        t_board_O.reset_board()\n",
    "\n",
    "        #X lands on empty board\n",
    "        S_X = t_board_X.board_to_state()\n",
    "        \n",
    "        #X plays first\n",
    "        eps = eps_(train_X,i)\n",
    "        \n",
    "            \n",
    "        x_action = t_board_X.pick_best_action(Q_X,action_type = X_action_type,eps=eps)\n",
    "        x_action1d = t_board_X.b2_to_s1[x_action]\n",
    "        \n",
    "        R_X = t_board_X.my_move(x_action) # make move on X's board\n",
    "        t_board_O.opponent_move(x_action) # make same move on O's board\n",
    "\n",
    "        while not (t_board_X.is_game_over() or t_board_O.is_game_over()):\n",
    "            S_O = t_board_O.board_to_state()\n",
    "            \n",
    "            #O plays second\n",
    "            eps = eps_(train_O,i)\n",
    "            \n",
    "            \n",
    "            o_action = t_board_O.pick_best_action(Q_O,action_type=O_action_type,eps=eps)\n",
    "            o_action1d = t_board_O.b2_to_s1[o_action]\n",
    "            R_O = t_board_O.my_move(o_action) #make move on O's board\n",
    "            t_board_X.opponent_move(o_action) #make same move on X's board\n",
    "            if  t_board_O.is_game_over(): \n",
    "                #need to end game here if O makes the winnng move and add a reward \n",
    "                if train_O:\n",
    "                    Q_O[S_O][o_action1d] += alpha*(R_O + 0 - Q_O[S_O][o_action1d]) # 0 given value of terminal state is 0\n",
    "                \n",
    "                if train_X:\n",
    "                #Need to penalize X's previous action if game is over\n",
    "                    Q_X[S_X][x_action1d] += alpha*(-R_O + 0 - Q_X[S_X][x_action1d]) \n",
    "                \n",
    "                rewards_O.append(R_O)\n",
    "                rewards_X.append(-R_O)\n",
    "                break\n",
    "            \n",
    "            S_X_new = t_board_X.board_to_state() #Get new state\n",
    "            #Calculate max_a Q_X(S',a)\n",
    "            if train_X:\n",
    "                x_action_ = t_board_X.pick_best_action(Q_X,action_type = 'greedy',eps=0.05) #best action from S_new\n",
    "                x_action_1d = t_board_X.b2_to_s1[x_action_]\n",
    "                Q_X[S_X][x_action1d]+= alpha*(R_X + gamma*Q_X[S_X_new][x_action_1d] - Q_X[S_X][x_action1d])\n",
    "        \n",
    "            S_X = S_X_new\n",
    "    \n",
    "            \n",
    "\n",
    "            # X plays next\n",
    "            eps = eps_(train_X,i)\n",
    "            x_action = t_board_X.pick_best_action(Q_X,action_type = X_action_type,eps=eps)\n",
    "            x_action1d = t_board_X.b2_to_s1[x_action]\n",
    "            R_X = t_board_X.my_move(x_action) #make move on X's board\n",
    "            t_board_O.opponent_move(x_action) #make same move on O's board\n",
    "\n",
    "            if t_board_X.is_game_over(): \n",
    "                if train_O:\n",
    "                    #need to end game here if X makes the winning move and make sure O's action is penalized\n",
    "                    Q_O[S_O][o_action1d] += alpha*(-R_X + 0 - Q_O[S_O][o_action1d]) #0 given value of terminal state is 0\n",
    "                \n",
    "                if train_X:\n",
    "                    #need to end game here if X makes the winning move and make sure reward is added to V\n",
    "                    Q_X[S_X][x_action1d] += alpha*(R_X + 0 - Q_X[S_X][x_action1d]) #0 given value of terminal state is 0\n",
    "                \n",
    "                rewards_X.append(R_X)\n",
    "                rewards_O.append(-R_X)\n",
    "                break   \n",
    "\n",
    "\n",
    "            S_O_new = t_board_O.board_to_state() #Get new state\n",
    "            #Calculate max_a Q_O(S',a)\n",
    "            if train_O:\n",
    "                o_action_ = t_board_O.pick_best_action(Q_O,action_type = 'greedy',eps=0.05) #best action from S_new\n",
    "                o_action_1d = t_board_O.b2_to_s1[o_action_]\n",
    "                Q_O[S_O][o_action1d]+= alpha*(R_O + gamma*Q_O[S_O_new][o_action_1d] - Q_O[S_O][o_action1d])\n",
    "\n",
    "            S_O = S_O_new\n",
    "            \n",
    "    if train_X:\n",
    "        rewards = rewards_X\n",
    "    elif train_O:\n",
    "        rewards = rewards_O\n",
    "        \n",
    "\n",
    "        \n",
    "    return Q_X,Q_O,rewards_X,rewards_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "\n",
    "t_board_X = TicTacToe(player = 'X',reward_type ='goal_reward')\n",
    "t_board_O = TicTacToe(player = 'O',reward_type ='goal_reward')\n",
    "\n",
    "States_X = t_board_X.my_states\n",
    "States_O = t_board_O.my_states\n",
    "\n",
    "Q_X = initialize_Q(States_X)\n",
    "Q_O = initialize_Q(States_O)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:03<00:00, 322.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Win Rate: 61.6 %\n",
      "O Win Rate: 28.4 %\n",
      "Draws Rate: 10.0 %\n",
      "Best Starting Move of X: (1, 1)\n",
      "Best Starting Move of O: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Random X vs Random O\n",
    "#by setting X and O to eps greedy and epsilon=1 we ensure that the agents only play random moves\n",
    "\n",
    "\n",
    "n_games = 1000\n",
    "\n",
    "result, initial_winning_move_X, initial_winning_move_O = play_games(n_games, Q_X, Q_O, X_strategy='eps_greedy', O_strategy='eps_greedy', eps_X=1, eps_O=1)\n",
    "\n",
    "print(\"X Win Rate:\", result['X_win']/n_games*100,'%')\n",
    "print(\"O Win Rate:\", result['O_win']/n_games*100,'%')\n",
    "print(\"Draws Rate:\", result['Draw']/n_games*100,'%')\n",
    "\n",
    "print(\"Best Starting Move of X:\", initial_winning_move_X)\n",
    "print(\"Best Starting Move of O:\", initial_winning_move_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:49<00:00, 406.74it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 458.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Win Rate: 96.0 %\n",
      "O Win Rate: 0.0 %\n",
      "Draws Rate: 4.0 %\n",
      "Best Starting Move of X: (0, 0)\n",
      "Best Starting Move of O: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# X Trained with random O vs random O\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "Q_X,_,rewards_X,rewards_O = train(n_games=20000,alpha = 0.2, gamma = 0.9,train_X=True,train_O=False,is_random=True)\n",
    "Q_X_trained = Q_X\n",
    "\n",
    "n_games=1000\n",
    "result, initial_winning_move_X, initial_winning_move_O = play_games(n_games, Q_X_trained, Q_O, X_strategy='greedy', O_strategy='eps_greedy', eps_X=1, eps_O=1)\n",
    "\n",
    "print(\"X Win Rate:\", result['X_win']/n_games*100,'%')\n",
    "print(\"O Win Rate:\", result['O_win']/n_games*100,'%')\n",
    "print(\"Draws Rate:\", result['Draw']/n_games*100,'%')\n",
    "\n",
    "print(\"Best Starting Move of X:\", initial_winning_move_X)\n",
    "print(\"Best Starting Move of O:\", initial_winning_move_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:56<00:00, 356.29it/s]\n",
      "100%|██████████| 1000/1000 [00:02<00:00, 461.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Win Rate: 9.6 %\n",
      "O Win Rate: 82.8 %\n",
      "Draws Rate: 7.6 %\n",
      "Best Starting Move of X: (1, 2)\n",
      "Best Starting Move of O: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Random X vs O Trained with random X\n",
    "#again eps set to 1 and eps greedy for x player this time\n",
    "np.random.seed(1)\n",
    "_,Q_O,rewards_X,rewards_O = train(n_games=20000,alpha = 0.2, gamma = 0.5,train_X=False,train_O=True,is_random=True)\n",
    "Q_O_trained = Q_O\n",
    "\n",
    "n_games=1000\n",
    "result, initial_winning_move_X, initial_winning_move_O = play_games(n_games,Q_X,Q_O_trained,X_strategy = 'eps_greedy',O_strategy='greedy',eps_X=1.0,\n",
    "                   eps_O=1.0)\n",
    "\n",
    "print(\"X Win Rate:\", result['X_win']/n_games*100,'%')\n",
    "print(\"O Win Rate:\", result['O_win']/n_games*100,'%')\n",
    "print(\"Draws Rate:\", result['Draw']/n_games*100,'%')\n",
    "\n",
    "print(\"Best Starting Move of X:\", initial_winning_move_X)\n",
    "print(\"Best Starting Move of O:\", initial_winning_move_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 464.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Win Rate: 100.0 %\n",
      "O Win Rate: 0.0 %\n",
      "Draws Rate: 0.0 %\n",
      "Best Starting Move of X: (0, 0)\n",
      "Best Starting Move of O: (0, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Trained X vs Trained O (both trained agaist random)\n",
    "#both greedy, eps are irrelevant due to the qtable already uptodate\n",
    "n_games=1000\n",
    "result, initial_winning_move_X, initial_winning_move_O = play_games(n_games,Q_X_trained,Q_O_trained,X_strategy = 'greedy',O_strategy='greedy',eps_X=1.0,\n",
    "                   eps_O=1.0)\n",
    "\n",
    "print(\"X Win Rate:\", result['X_win']/n_games*100,'%')\n",
    "print(\"O Win Rate:\", result['O_win']/n_games*100,'%')\n",
    "print(\"Draws Rate:\", result['Draw']/n_games*100,'%')\n",
    "\n",
    "print(\"Best Starting Move of X:\", initial_winning_move_X)\n",
    "print(\"Best Starting Move of O:\", initial_winning_move_O)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
